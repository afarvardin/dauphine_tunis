# countByWindow
Return a new DStream in which each RDD has a single element generated by counting the number of elements in a window over this DStream. windowDuration and slideDuration are as defined in the window() operation.
> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.DStream.countByWindow.html

ssc.checkpoint("checkpoint") 
val lines = ssc.socketTextStream("localhost", 9999) 
val words = lines flatMap {line => line.split(" ")} 
val windowLen = 30 
val slidingInterval = 10 
val countByWindow = words.countByWindow(Seconds(windowLen), Seconds(slidingInterval)) 
countByWindow.print()


# countByValueAndWindow
Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream.
> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.DStream.countByValueAndWindow.html

ssc.checkpoint("checkpoint") 
val lines = ssc.socketTextStream("localhost", 9999) 
val words = lines flatMap {line => line.split(" ")} 
val windowLen = 30 
val slidingInterval = 10 
val countByValueAndWindow = words.countByValueAndWindow(
	Seconds(windowLen), 
	Seconds(slidingInterval)) 
countByValueAndWindow.print()


# From the link below try _countByValue_:
Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream.
> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.DStream.countByValue.html

Try this practice your your strength as well.


